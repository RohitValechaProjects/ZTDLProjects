{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification\n",
    "\n",
    "In this exercise, you will get familiar with how to build RNNs in Keras. You will build a recurrent model to classify moview reviews as either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Sentiment Dataset\n",
    "\n",
    "The large movie review dataset is a collection of 25k positive and 25k negative movie reviews from [IMDB](http://www.imdb.com). Here are some excerpts from the dataset, both easy and hard, to get a sense of why this dataset is challenging:\n",
    "\n",
    "> Ah, I loved this movie.\n",
    "\n",
    "> Quite honestly, The Omega Code is the worst movie I have seen in a very long time.\n",
    "\n",
    "> The wit and pace and three show stopping Busby Berkley numbers put this ahead of the over-rated 42nd Street. \n",
    "\n",
    "> There simply was no suspense, precious little excitement and too many dull spots, most of them trying to show why \"Nellie\" (Monroe) was so messed up.\n",
    "\n",
    "The dataset can be found at http://ai.stanford.edu/~amaas/data/sentiment/. Since this is a common dataset for RNNs, Keras has a preprocessed version built-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will limit to the most frequent 20k words defined by max_features, our vocabulary size\n",
    "max_features = 100\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is preprocessed by replacing words with indexes - review [Keras's docs](http://keras.io/datasets/#imdb-movie-reviews-sentiment-classification). Here's the first review in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 65,\n",
       " 2,\n",
       " 2,\n",
       " 66,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 36,\n",
       " 2,\n",
       " 5,\n",
       " 25,\n",
       " 2,\n",
       " 43,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 39,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 17,\n",
       " 2,\n",
       " 38,\n",
       " 13,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 2,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 2,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 2,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 2,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 2,\n",
       " 12,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 16,\n",
       " 2,\n",
       " 66,\n",
       " 2,\n",
       " 33,\n",
       " 4,\n",
       " 2,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 2,\n",
       " 5,\n",
       " 25,\n",
       " 2,\n",
       " 51,\n",
       " 36,\n",
       " 2,\n",
       " 48,\n",
       " 25,\n",
       " 2,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 2,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 2,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 15,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 2,\n",
       " 2,\n",
       " 26,\n",
       " 2,\n",
       " 2,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 13,\n",
       " 2,\n",
       " 88,\n",
       " 4,\n",
       " 2,\n",
       " 15,\n",
       " 2,\n",
       " 98,\n",
       " 32,\n",
       " 2,\n",
       " 56,\n",
       " 26,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 18,\n",
       " 4,\n",
       " 2,\n",
       " 22,\n",
       " 21,\n",
       " 2,\n",
       " 2,\n",
       " 26,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 30,\n",
       " 2,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 2,\n",
       " 92,\n",
       " 25,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 2,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 2,\n",
       " 5,\n",
       " 16,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 19,\n",
       " 2,\n",
       " 32]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = X_train[0]\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convince ourselves that these are movies reviews, using the vocabulary provided by keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a dictionary from index to word, notice that words are indexed starting from the number 3, while the first three entries are for special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {i+3: w for w, i in word_index.items()}\n",
    "index_word[0]=''\n",
    "index_word[1]='start_char'\n",
    "index_word[2]='oov'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can covert the first review to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"start_char this film was just oov oov oov oov story oov oov really oov the oov they oov and you oov just oov oov there oov oov is an oov oov and oov the oov oov oov oov oov oov from the oov oov oov as oov so i oov the oov there was a oov oov with this film the oov oov oov the film were great it was just oov so much that i oov the film as oov as it was oov for oov and would oov it to oov to oov and the oov oov was oov really oov at the oov it was so oov and you oov what they oov if you oov at a film it oov have been good and this oov was also oov to the oov oov oov that oov the oov of oov and oov they were just oov oov are oov oov out of the oov oov i oov because the oov that oov them all oov up are oov a oov oov for the oov film but oov oov are oov and oov be oov for what they have oov don't you oov the oov story was so oov because it was oov and was oov oov oov all that was oov with oov all\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([index_word[i] for i in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - prepare the data\n",
    "\n",
    "The reviews are different lengths but we need to fit them into a matrix to feed to Keras. We will do this by picking a maximum word length and cutting off words from the examples that are over that limit and padding the examples with 0 if they are under the limit.\n",
    "\n",
    "Refer to the [Keras docs](http://keras.io/preprocessing/sequence/#pad_sequences) for the `pad_sequences` function. Use `pad_sequences` to prepare both `X_train` and `X_test` to be `maxlen` long at the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 80)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 80\n",
    "# Pad and clip the example sequences\n",
    "X_train_t = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_t = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "X_train_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - build an RNN for classifying reviews as positive or negative\n",
    "\n",
    "Build a single-layer RNN model and train it. You will need to include these parts:\n",
    "\n",
    "* An `Embedding` layer for efficiently one-hot encoding the inputs - [docs](http://keras.io/layers/embeddings/)\n",
    "* A recurrent layer. Keras has a [few variants](http://keras.io/layers/recurrent/) you could use. LSTM layers are by far the most popular for RNNs.\n",
    "* A `Dense` layer for the hidden to output connection.\n",
    "* A softmax to produce the final prediction.\n",
    "\n",
    "You will need to decide how large your hidden state will be. You may also consider using some dropout on your recurrent or embedding layers - refers to docs for how to do this.\n",
    "\n",
    "Training for longer will be much better overall, but since RNNs are expensive to train, you can use 1 epoch to test. You should be able to get > 70% accuracy with 1 epoch. How high can you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 80, 100)           10000     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 12)                5424      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 15,437\n",
      "Trainable params: 15,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Design an recurrent model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=100, input_length=maxlen))\n",
    "model.add(LSTM(12))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "25000/25000 [==============================] - 16s 622us/sample - loss: 0.5875 - accuracy: 0.6851 - val_loss: 0.5782 - val_accuracy: 0.6950\n",
      "25000/25000 [==============================] - 5s 189us/sample - loss: 0.5782 - accuracy: 0.6950\n",
      "Test loss: 0.5781713641166687\n",
      "Test accuracy: 0.695\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_t, y_train, batch_size=32, epochs=1, validation_data=(X_test_t, y_test))\n",
    "loss, acc = model.evaluate(X_test_t, y_test, batch_size=32)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
