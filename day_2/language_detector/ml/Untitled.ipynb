{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build a language detector model\n",
    "\n",
    "The goal of this exercise is to train a linear classifier on text features\n",
    "that represent sequences of up to N consecutive characters so as to be\n",
    "recognize natural languages by using the frequencies of short character\n",
    "sequences as 'fingerprints'.\n",
    "\n",
    "The script saves the trained model to disk for later use\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "# Adapted by: Francesco Mosconi\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# The training data folder must be passed as first argument\n",
    "try:\n",
    "    dataset = load_files('./wikidata/short_paragraphs')\n",
    "except OSError as ex:\n",
    "    print(ex)\n",
    "    print(\"Couldn't import the data, did you unzip the wikidata.zip folder?\")\n",
    "    exit(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ar', 'de', 'en', 'es', 'fr', 'it', 'nl', 'pl', 'pt', 'ru']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Split the dataset in training and test set\n",
    "# (use 20% of the data for test):\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer that splits\n",
    "# strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "# using the class TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1, 5), analyzer='char_wb')\n",
    "#vec = TfidfVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "#vec = TfidfVectorizer(ngram_range=(1, 20), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Use the function make_pipeline to build a\n",
    "#       vectorizer / classifier pipeline\n",
    "#       using the previous analyzer\n",
    "#       and a classifier of choice.\n",
    "#       The pipeline instance should be\n",
    "#       stored in a variable named model\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#est = DecisionTreeClassifier()\n",
    "#est = LogisticRegression()\n",
    "est = LogisticRegression(C=10)\n",
    "model = make_pipeline(vec, est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='char_wb', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 5), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: Fit the pipeline on the training set\n",
    "\n",
    "model.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='char_wb', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 5), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: Fit the pipeline on the training set\n",
    "\n",
    "model.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Predict the outcome on the testing set.\n",
    "# Store the result in a variable named y_predicted\n",
    "\n",
    "y_predicted = model.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        44\n",
      "           1       0.99      0.99      0.99       206\n",
      "           2       0.95      0.99      0.97       223\n",
      "           3       0.97      0.97      0.97       228\n",
      "           4       0.98      0.99      0.99       189\n",
      "           5       1.00      0.98      0.99       208\n",
      "           6       0.99      0.95      0.97        83\n",
      "           7       0.97      0.98      0.97       127\n",
      "           8       0.94      0.95      0.95       195\n",
      "           9       1.00      0.98      0.99       189\n",
      "\n",
      "    accuracy                           0.98      1692\n",
      "   macro avg       0.98      0.98      0.98      1692\n",
      "weighted avg       0.98      0.98      0.98      1692\n",
      "\n",
      "    ar   de   en   es   fr   it  nl   pl   pt   ru\n",
      "ar  43    0    1    0    0    0   0    0    0    0\n",
      "de   0  203    0    0    0    0   0    3    0    0\n",
      "en   0    1  221    1    0    0   0    0    0    0\n",
      "es   0    0    0  221    0    1   0    0    6    0\n",
      "fr   0    0    1    0  187    0   0    0    1    0\n",
      "it   0    0    1    0    1  203   1    0    2    0\n",
      "nl   0    1    2    0    1    0  79    0    0    0\n",
      "pl   0    0    2    0    0    0   0  124    1    0\n",
      "pt   0    0    2    5    1    0   0    1  186    0\n",
      "ru   0    0    2    0    0    0   0    0    1  186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# TASK: Print the classification report\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "# TASK: Print the confusion matrix. Bonus points if you make it pretty\n",
    "cr = classification_report(y_test, y_predicted)\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "cmdf = pd.DataFrame(cm, index=dataset.target_names, columns=dataset.target_names)\n",
    "print(cmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Split the dataset in training and test set\n",
    "# (use 20% of the data for test):\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y, test_size=.2, random_state=0)\n",
    "\n",
    "# TASK: Build a an vectorizer that splits\n",
    "# strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "# using the class TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "\n",
    "# TASK: Use the function make_pipeline to build a\n",
    "#       vectorizer / classifier pipeline\n",
    "#       using the previous analyzer\n",
    "#       and a classifier of choice.\n",
    "#       The pipeline instance should be\n",
    "#       stored in a variable named model\n",
    "est = DecisionTreeClassifier()\n",
    "model = make_pipeline(vec, est)\n",
    "\n",
    "# TASK: Fit the pipeline on the training set\n",
    "\n",
    "\n",
    "# TASK: Predict the outcome on the testing set.\n",
    "# Store the result in a variable named y_predicted\n",
    "\n",
    "\n",
    "# TASK: Print the classification report\n",
    "\n",
    "\n",
    "# TASK: Print the confusion matrix. Bonus points if you make it pretty.\n",
    "\n",
    "\n",
    "# TASK: Is the score good? Can you improve it changing\n",
    "#       the parameters or the classifier?\n",
    "#       Try using cross validation and grid search\n",
    "\n",
    "# TASK: Use dill and gzip to persist the trained model in memory.\n",
    "#       1) gzip.open a file called my_model.dill.gz\n",
    "#       2) dump to the file both your trained classifier\n",
    "#          and the target_names of the dataset (for later use)\n",
    "#    They should be passed as a list [model, dataset.target_names]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
